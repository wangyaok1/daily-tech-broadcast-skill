#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¯æ—¥ç§‘æŠ€æ’­æŠ¥ - ä»å¯æŠ“å–ä¿¡æ¯æºæ‹‰å–æ ‡é¢˜å¹¶ç”Ÿæˆç®€æŠ¥ã€‚
ä»…å°†ç®€æŠ¥æ­£æ–‡è¾“å‡ºåˆ° stdoutï¼Œè°ƒè¯•ä¿¡æ¯è¾“å‡ºåˆ° stderrã€‚
æ— éœ€ API Keyï¼Œæ”¯æŒ UTF-8ã€‚
"""

import sys
import re
from datetime import datetime
from urllib.request import urlopen, Request
from urllib.error import URLError, HTTPError
from urllib.parse import urljoin

# ä¿¡æ¯æºåˆ—è¡¨ï¼ˆå¯é…ç½®ã€å¯æ‰©å±•ï¼‰ï¼šåç§° -> é¦–é¡µ URL
# ä¼˜å…ˆä½¿ç”¨å·²éªŒè¯å¯æŠ“å–ã€æ— éœ€ API Key çš„æº
NEWS_SOURCES = [
    {"name": "æ–°æµªç§‘æŠ€", "url": "https://tech.sina.com.cn/"},
    {"name": "ITä¹‹å®¶", "url": "https://www.ithome.com/"},
]

# ç®€æŠ¥æœ€å¤šæ¡æ•°
MAX_ITEMS = 12
# è¯·æ±‚è¶…æ—¶ï¼ˆç§’ï¼‰
TIMEOUT = 15
# User-Agentï¼Œé¿å…è¢«æ‹’
UA = "Mozilla/5.0 (compatible; DailyTechBroadcast/1.0)"


def log(msg: str) -> None:
    """è°ƒè¯•ä¿¡æ¯å†™åˆ° stderrï¼Œä¸æ±¡æŸ“ stdoutã€‚"""
    print(msg, file=sys.stderr)


def fetch_url(url: str) -> str | None:
    """æŠ“å– URL è¿”å› HTML æ–‡æœ¬ï¼Œå¤±è´¥è¿”å› Noneã€‚"""
    try:
        req = Request(url, headers={"User-Agent": UA})
        with urlopen(req, timeout=TIMEOUT) as resp:
            raw = resp.read()
            # å°è¯•å¸¸è§ç¼–ç 
            for enc in ("utf-8", "gbk", "gb2312"):
                try:
                    return raw.decode(enc, errors="replace")
                except (LookupError, UnicodeDecodeError):
                    continue
            return raw.decode("utf-8", errors="replace")
    except (URLError, HTTPError, OSError) as e:
        log(f"æŠ“å–å¤±è´¥ {url}: {e}")
        return None


def _is_article_link(href: str) -> bool:
    """åˆ¤æ–­æ˜¯å¦ä¸ºæ–‡ç« é“¾æ¥ï¼ˆæ’é™¤å¯¼èˆªã€è¯„è®ºç­‰ï¼‰ã€‚"""
    if not href or "comment" in href or "javascript:" in href:
        return False
    if "/doc-" in href or "/tech/" in href or "finance.sina.com.cn/tech" in href:
        return True
    if "sina.com.cn" in href and ("doc-" in href or "article" in href):
        return True
    return False


def _is_nav_or_short(text: str) -> bool:
    """è¿‡æ»¤å¯¼èˆªã€çŸ­æ ‡ç­¾ã€‚"""
    if not text or len(text) < 8:
        return True
    nav = ("é¦–é¡µ", "å®¢æˆ·ç«¯", "å¾®åš", "è§†é¢‘", "ä½“è‚²", "è´¢ç»", "åšå®¢", "æ¸¸æˆ", "ä¼—æµ‹", "GIF", "ç§‘å­¦å¤§å®¶", "æ–°æµª")
    return any(text == x or text.startswith(x) for x in nav)


def extract_headlines_sina(html: str, base_url: str) -> list[dict]:
    """ä»æ–°æµªç§‘æŠ€é¦–é¡µæå–æ–°é—»æ ‡é¢˜ä¸é“¾æ¥ï¼ˆä¼˜å…ˆæ–‡ç« é“¾æ¥ï¼‰ã€‚"""
    from html.parser import HTMLParser

    class LinkParser(HTMLParser):
        def __init__(self):
            super().__init__()
            self.links = []
            self._in_a = False
            self._current_href = ""
            self._current_text = []

        def handle_starttag(self, tag, attrs):
            if tag == "a":
                self._in_a = True
                self._current_href = ""
                self._current_text = []
                for k, v in attrs:
                    if k == "href" and v:
                        self._current_href = v if v.startswith("http") else urljoin(base_url, v)
                        break

        def handle_endtag(self, tag):
            if tag == "a" and self._in_a:
                self._in_a = False
                text = "".join(self._current_text).strip()
                if not text or not self._current_href:
                    return
                if _is_nav_or_short(text):
                    return
                if len(text) > 120:
                    return
                if not re.search(r"[\u4e00-\u9fff]", text):
                    return
                href = self._current_href
                if "sina.com" not in href or "comment" in href:
                    return
                self.links.append({"title": text, "url": href, "is_article": _is_article_link(href)})

        def handle_data(self, data):
            if self._in_a:
                self._current_text.append(data)

    p = LinkParser()
    try:
        p.feed(html)
    except Exception:
        return []
    # ä¼˜å…ˆæ–‡ç« é“¾æ¥ï¼Œå†æŒ‰æ ‡é¢˜å»é‡
    articles = [{"title": x["title"], "url": x["url"]} for x in p.links if x.get("is_article")]
    others = [{"title": x["title"], "url": x["url"]} for x in p.links if not x.get("is_article")]
    combined = articles + others
    seen = set()
    out = []
    for item in combined:
        t = item["title"]
        if t not in seen:
            seen.add(t)
            out.append(item)
    return out[:MAX_ITEMS]


def extract_headlines_ithome(html: str, base_url: str) -> list[dict]:
    """ä» ITä¹‹å®¶ é¦–é¡µæå–æ ‡é¢˜ä¸é“¾æ¥ã€‚"""
    from html.parser import HTMLParser

    class LinkParser(HTMLParser):
        def __init__(self):
            super().__init__()
            self.links = []
            self._in_a = False
            self._current_href = ""
            self._current_text = []

        def handle_starttag(self, tag, attrs):
            if tag == "a":
                self._in_a = True
                self._current_href = ""
                self._current_text = []
                for k, v in attrs:
                    if k == "href" and v:
                        self._current_href = v if v.startswith("http") else urljoin(base_url, v)
                        break

        def handle_endtag(self, tag):
            if tag == "a" and self._in_a:
                self._in_a = False
                text = "".join(self._current_text).strip()
                if text and 4 <= len(text) <= 100 and "ithome.com" in self._current_href:
                    self.links.append({"title": text, "url": self._current_href})

        def handle_data(self, data):
            if self._in_a:
                self._current_text.append(data)

    p = LinkParser()
    try:
        p.feed(html)
    except Exception:
        return []
    seen = set()
    out = []
    for item in p.links:
        t = item["title"]
        if t not in seen and re.search(r"[\u4e00-\u9fff]", t):
            seen.add(t)
            out.append(item)
    return out[:MAX_ITEMS]


def fetch_news() -> list[dict]:
    """ä»é…ç½®çš„ä¿¡æ¯æºæŠ“å–ï¼Œè¿”å› [{title, url, source}]ï¼Œå¤±è´¥é™çº§ä¸æŠ›å¼‚å¸¸ã€‚"""
    all_items = []
    for src in NEWS_SOURCES:
        name, url = src["name"], src["url"]
        html = fetch_url(url)
        if not html:
            continue
        if "sina" in url.lower():
            items = extract_headlines_sina(html, url)
        elif "ithome" in url.lower():
            items = extract_headlines_ithome(html, url)
        else:
            items = []
        for it in items:
            it["source"] = name
            all_items.append(it)
        if len(all_items) >= MAX_ITEMS:
            break
    # å»é‡ï¼ˆæŒ‰æ ‡é¢˜ï¼‰
    seen = set()
    unique = []
    for it in all_items:
        if it["title"] not in seen:
            seen.add(it["title"])
            unique.append(it)
    return unique[:MAX_ITEMS]


def generate_broadcast(items: list[dict]) -> str:
    """ç”Ÿæˆç®€æŠ¥çº¯æ–‡æœ¬ï¼ˆUTF-8ï¼‰ã€‚"""
    today = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")
    lines = [
        f"# ğŸ“° æ¯æ—¥ç§‘æŠ€æ–°é—»ç®€æŠ¥",
        f"**{today}**",
        "",
    ]
    if not items:
        lines.extend(["æš‚æ— æŠ“å–åˆ°çš„æ–°é²œæ¡ç›®ï¼Œè¯·ç¨åå†è¯•æˆ–æ£€æŸ¥ç½‘ç»œã€‚", ""])
    else:
        for i, it in enumerate(items, 1):
            lines.append(f"{i}. **{it['title']}**")
            lines.append(f"   æ¥æºï¼š{it.get('source', 'ç§‘æŠ€åª’ä½“')}")
            lines.append("")
    lines.append("---")
    lines.append("*ç”±æ¯æ—¥ç§‘æŠ€æ’­æŠ¥ Skill ç”Ÿæˆï¼Œæ— éœ€ API Keyã€‚*")
    return "\n".join(lines)


def main() -> int:
    """ä»…å°†ç®€æŠ¥è¾“å‡ºåˆ° stdoutï¼›é”™è¯¯ä¿¡æ¯åˆ° stderrã€‚"""
    try:
        items = fetch_news()
        text = generate_broadcast(items)
        # ä»… stdout è¾“å‡ºç®€æŠ¥ï¼Œä¾› cron/æ¶ˆæ¯å‘é€ä½¿ç”¨
        print(text, flush=True)
        return 0
    except Exception as e:
        log(f"broadcast å¼‚å¸¸: {e}")
        # ä»è¾“å‡ºä¸€æ®µé™çº§æ–‡æ¡ˆï¼Œé¿å…å®Œå…¨ç©ºç™½
        fallback = f"# ğŸ“° æ¯æ—¥ç§‘æŠ€æ–°é—»ç®€æŠ¥\n\nä»Šæ—¥ç®€æŠ¥ç”Ÿæˆæ—¶é‡åˆ°é—®é¢˜ï¼š{e}\nè¯·ç¨åé‡è¯•æˆ–æ£€æŸ¥ç½‘ç»œã€‚"
        print(fallback, flush=True)
        return 0  # è¿”å› 0 è®©ä¸Šå±‚ä»èƒ½å‘é€è¿™æ¡é™çº§æ¶ˆæ¯


if __name__ == "__main__":
    sys.exit(main())
